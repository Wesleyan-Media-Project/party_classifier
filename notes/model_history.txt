1. 80/20 ad-level split, acb only
-- means that ads from the same entities end up in different splits
-- sort of okay-ish given that only acb was used and ads were deduplicated prior to the split
-- but when other fields (namely page_name and disclaimer) are used as well, this means we're effectively leaking information into the test set

2. pd_id-level split
-- addresses this problem
-- however, problem still partially exists because one page_id can have multiple pd_ids which will still share page_name and disclaimer

2.1
-- at this point, Erika asked for a comparison between 1 and 2
-- rather than use the respective test sets I had created and used, she wanted us to apply it to the whole 1.18m datasetand all ads for which party_all had already been coded
-- this exacerbates the problem with pd_ids ending in both training and test because we now had a secondary test set which had already partially been trained on fully, and partially had already contained the same pages in the training set

3. page_id-level split
-- solves the pd_id-level problem
-- however, I had trained on a dataset that didn't contain all 1.18m ads, when I thought it did. Hence, some of the pages were still split between the training set and the inference set, and given the goal of using the latter for comparison, that again made for an unfair comparison

4. page_id-level split based on merge of 1.18m and most recent WMP entity file
-- solves that problem
-- additional problem that happens when using page_id is that party_all is coded at the pd_id level. So it's possible for some versions of a page to be Democratic, and others to be Other (happens only once). More commonly (112 pages), some pd_ids are coded as DEM/REP and the others as MISSING
-- If those end up in the training set the model will get confused because it won't be able to associated the shared features of those ads with a single target

5. page_id-level split as above, with only page_ids whose pd_ids all share the same party (DEM/REP)
-- solves the above


remaining issues:
-- these 5 classifiers aren't really directly comparable because they're trained on different things. It would be like comparing the log-likelihood of several regression models on different variables, which we also wouldn't do!








